*****************************************************
* Dataset Sizes
*****************************************************
Training samples: 18334
Validation samples: 2284
*****************************************************
* Best Results
*****************************************************
Best validation accuracy: 0.7211
Best Epoch: 22
Best training loss: 0.2927
Best validation loss: 3.2159

*****************************************************
* Hyperparameters
*****************************************************
lr: 0.0005
batch_size: 64
epochs: 25
weight_decay: 0.05
hidden_dim: 256
embedding_dim: 256
num_layers: 2
dropout: 0.5

*****************************************************
* Best Classification Report
*****************************************************
Classification Report for Epoch 22:
                 precision    recall  f1-score   support

  GameofThrones       0.56      0.60      0.58       116
       StarWars       0.22      0.30      0.26        96
        Tolkien       0.64      0.59      0.61       193
     Historical       0.92      0.84      0.88      1229
      DoctorWho       0.10      0.28      0.15        18
      Mythology       0.65      0.65      0.65       322
ForgottenRealms       0.06      0.10      0.08        21
       Warcraft       0.18      0.14      0.16        14
      Offensive       0.90      0.94      0.92       153
    HarryPotter       0.38      0.44      0.41        81
   FinalFantasy       0.00      0.00      0.00         7
     DragonBall       0.09      0.20      0.12        10
         Naruto       0.24      0.58      0.34        12
     TheWitcher       0.00      0.00      0.00         7
      Discworld       0.00      0.00      0.00         5

       accuracy                           0.72      2284
      macro avg       0.33      0.38      0.34      2284
   weighted avg       0.76      0.72      0.74      2284


*****************************************************
* SK Learn Accuracy score
*****************************************************
Best score: 0.7211


*****************************************************
* Data Configuration
*****************************************************
*****************************************************
* Data Processing Configuration
*****************************************************
path: dataset/preprocessing
output_file: dataset.txt
train_file: train.txt
dev_file: dev.txt
test_file: test.txt
config_dump: data_config.info
train_size: 0.8
dev_size: 0.1
test_size: 0.1
unique_names: True
augmentation:
  enabled: False
  only_basic_augmentation: False
  intensity: 3
  swap_characters: {'enabled': True, 'pct_words_to_swap': 0.6, 'transformations_per_example': {'min': 1, 'max': 5}}
  insert_characters: {'enabled': True, 'pct_words_to_swap': 0.6, 'transformations_per_example': {'min': 1, 'max': 5}}
  delete_characters: {'enabled': True, 'pct_words_to_swap': 0.6, 'transformations_per_example': {'min': 1, 'max': 4}}
  duplicate_characters: {'enabled': True, 'pct_words_to_swap': 0.6, 'transformations_per_example': 3}
  split_names: {'enabled': True, 'join_parts': True}
  label_exclusion: {'enabled': False, 'excluded_labels': []}
  internal_swap: {'enabled': True, 'swap_probability': 0.7}
labels: ['HarryPotter', 'StarWars', 'Tolkien', 'Warcraft', 'DragonBall', 'Naruto', 'ForgottenRealms', 'FinalFantasy', 'GameofThrones', 'TheWitcher', 'DoctorWho', 'Discworld', 'Mythology', 'Offensive', 'Historical']

Datasets:
  - Wikidata:
      name: Wikidata
      path: wikidata
      input_folder: raw_data
      output_folder: processed_data
      labels_folder: labels
      labels_file: labels.txt
      historical_file: extremely_relevant_figures2.txt
      dataset_file: wikidata-universes.csv
      output_file: wikidata_dataset_FastText.txt
  - Mythdata:
      name: Mythdata
      path: mythology
      input_folder: raw_data
      output_folder: processed_data
      dataset_file: myth_dataset.csv
      output_file: myth_dataset.txt
  - NERdata:
      name: NERdata
      path: ner
      input_folder: raw_data
      output_folder: processed_data
      output_file: ner_dataset.txt
  - Slurs:
      name: Slurs
      path: slurs
      input_folder: raw_data
      output_folder: processed_data
      dataset_file: profanity.csv
      output_file: slurs.txt


*****************************************************
* Epoch Output
*****************************************************
Epoch 1:
  Training Loss: 2.3652
  Validation Loss: 2.1942
  Validation Accuracy: 0.6095
  Learning Rate: 0.000500

Epoch 2:
  Training Loss: 1.9399
  Validation Loss: 2.0464
  Validation Accuracy: 0.4632
  Learning Rate: 0.000500

Epoch 3:
  Training Loss: 1.6901
  Validation Loss: 2.0665
  Validation Accuracy: 0.3459
  Learning Rate: 0.000500

Epoch 4:
  Training Loss: 1.4643
  Validation Loss: 2.1601
  Validation Accuracy: 0.5530
  Learning Rate: 0.000500

Epoch 5:
  Training Loss: 1.2397
  Validation Loss: 2.2521
  Validation Accuracy: 0.4641
  Learning Rate: 0.000250

Epoch 6:
  Training Loss: 0.9696
  Validation Loss: 2.2899
  Validation Accuracy: 0.5959
  Learning Rate: 0.000250

Epoch 7:
  Training Loss: 0.8212
  Validation Loss: 2.4047
  Validation Accuracy: 0.5871
  Learning Rate: 0.000250

Epoch 8:
  Training Loss: 0.7296
  Validation Loss: 2.4982
  Validation Accuracy: 0.6340
  Learning Rate: 0.000250

Epoch 9:
  Training Loss: 0.6571
  Validation Loss: 2.5322
  Validation Accuracy: 0.6292
  Learning Rate: 0.000250

Epoch 10:
  Training Loss: 0.5930
  Validation Loss: 2.6029
  Validation Accuracy: 0.6160
  Learning Rate: 0.000125

Epoch 11:
  Training Loss: 0.5119
  Validation Loss: 2.7428
  Validation Accuracy: 0.6690
  Learning Rate: 0.000125

Epoch 12:
  Training Loss: 0.4645
  Validation Loss: 2.8068
  Validation Accuracy: 0.6966
  Learning Rate: 0.000125

Epoch 13:
  Training Loss: 0.4411
  Validation Loss: 2.9079
  Validation Accuracy: 0.6655
  Learning Rate: 0.000125

Epoch 14:
  Training Loss: 0.4254
  Validation Loss: 2.8112
  Validation Accuracy: 0.6725
  Learning Rate: 0.000125

Epoch 15:
  Training Loss: 0.4062
  Validation Loss: 2.8407
  Validation Accuracy: 0.6681
  Learning Rate: 0.000063

Epoch 16:
  Training Loss: 0.3696
  Validation Loss: 3.0026
  Validation Accuracy: 0.6983
  Learning Rate: 0.000063

Epoch 17:
  Training Loss: 0.3451
  Validation Loss: 3.0589
  Validation Accuracy: 0.7163
  Learning Rate: 0.000063

Epoch 18:
  Training Loss: 0.3328
  Validation Loss: 3.0747
  Validation Accuracy: 0.7167
  Learning Rate: 0.000063

Epoch 19:
  Training Loss: 0.3300
  Validation Loss: 3.1299
  Validation Accuracy: 0.7145
  Learning Rate: 0.000063

Epoch 20:
  Training Loss: 0.3135
  Validation Loss: 3.2108
  Validation Accuracy: 0.7158
  Learning Rate: 0.000031

Epoch 21:
  Training Loss: 0.3008
  Validation Loss: 3.2401
  Validation Accuracy: 0.7194
  Learning Rate: 0.000031

Epoch 22:
  Training Loss: 0.2927
  Validation Loss: 3.2159
  Validation Accuracy: 0.7211
  Learning Rate: 0.000031

Epoch 23:
  Training Loss: 0.2817
  Validation Loss: 3.2271
  Validation Accuracy: 0.7145
  Learning Rate: 0.000031

Epoch 24:
  Training Loss: 0.2785
  Validation Loss: 3.3204
  Validation Accuracy: 0.7202
  Learning Rate: 0.000031

Epoch 25:
  Training Loss: 0.2787
  Validation Loss: 3.3199
  Validation Accuracy: 0.7141
  Learning Rate: 0.000016

*****************************************************
* Confusion Matrix
*****************************************************
[[  70,    2,   21,    3,    2,    6,    4,    2,    0,    1,    0,    2,
     0,    2,    1],
 [   6,   29,    4,   19,    0,   22,    1,    2,    2,    9,    0,    2,
     0,    0,    0],
 [  23,    7,  114,    3,    4,   17,    2,    4,    3,   10,    0,    2,
     3,    0,    1],
 [   9,   55,   10, 1027,   28,   46,    8,    0,    0,   24,    2,    1,
    11,    4,    4],
 [   0,    2,    0,    9,    5,    2,    0,    0,    0,    0,    0,    0,
     0,    0,    0],
 [  10,   24,   17,   18,    5,  209,   13,    0,    0,    7,    0,   12,
     6,    0,    1],
 [   2,    3,    3,    5,    0,    4,    2,    0,    0,    2,    0,    0,
     0,    0,    0],
 [   1,    1,    4,    1,    0,    4,    0,    2,    0,    0,    0,    0,
     0,    1,    0],
 [   0,    2,    1,    1,    0,    0,    0,    0,  144,    5,    0,    0,
     0,    0,    0],
 [   5,    4,    3,   16,    4,    2,    0,    0,   11,   36,    0,    0,
     0,    0,    0],
 [   0,    0,    0,    3,    0,    3,    1,    0,    0,    0,    0,    0,
     0,    0,    0],
 [   0,    1,    0,    2,    0,    3,    0,    0,    0,    0,    0,    2,
     2,    0,    0],
 [   0,    0,    0,    0,    0,    4,    0,    0,    0,    0,    0,    1,
     7,    0,    0],
 [   0,    1,    1,    3,    0,    1,    0,    0,    0,    0,    1,    0,
     0,    0,    0],
 [   0,    0,    0,    3,    0,    0,    0,    1,    0,    0,    0,    1,
     0,    0,    0]]
