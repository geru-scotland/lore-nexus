*****************************************************
* Best Results
*****************************************************
Best validation accuracy: 0.9636
Best Epoch: 26
Best training loss: 0.0403
Best validation loss: 0.1099

*****************************************************
* Hyperparameters
*****************************************************
lr: 0.001
batch_size: 32
epochs: 30
weight_decay: 0.03
hidden_dim: 512
embedding_dim: 128
num_layers: 2
dropout: 0.5

*****************************************************
* Best Classification Report
*****************************************************
Classification Report for Epoch 26:
                 precision    recall  f1-score   support

     Historical       0.99      0.98      0.98      3444
        Tolkien       0.94      0.96      0.95      4811
      Offensive       0.98      0.98      0.98      2979
       StarWars       0.97      0.97      0.97      4443
    HarryPotter       0.95      0.94      0.95      3376
      Mythology       0.91      0.90      0.91       805
      DoctorWho       0.95      0.94      0.95       720
   FinalFantasy       0.98      0.97      0.97       352
         Naruto       0.98      0.99      0.99       725
ForgottenRealms       0.95      0.93      0.94       523
  GameofThrones       0.97      0.97      0.97      4155
       Warcraft       0.98      0.97      0.98       459
     DragonBall       0.95      0.95      0.95       603
     TheWitcher       0.98      0.98      0.98       521
      Discworld       0.97      0.93      0.95       199

       accuracy                           0.96     28115
      macro avg       0.96      0.96      0.96     28115
   weighted avg       0.96      0.96      0.96     28115


*****************************************************
* SK Learn Accuracy score
*****************************************************
Best score: 0.9636


*****************************************************
* Data Configuration
*****************************************************
*****************************************************
* Data Processing Configuration
*****************************************************
path: dataset/preprocessing
output_file: dataset.txt
train_file: train.txt
dev_file: dev.txt
test_file: test.txt
config_dump: data_config.info
train_size: 0.8
dev_size: 0.1
test_size: 0.1
unique_names: False
augmentation:
  enabled: True
  intensity: 2
  swap_characters: {'enabled': True, 'pct_words_to_swap': 0.5, 'transformations_per_example': {'min': 1, 'max': 4}}
  insert_characters: {'enabled': True, 'pct_words_to_swap': 0.6, 'transformations_per_example': {'min': 1, 'max': 4}}
  delete_characters: {'enabled': True, 'pct_words_to_swap': 0.5, 'transformations_per_example': {'min': 1, 'max': 4}}
  duplicate_characters: {'enabled': True, 'pct_words_to_swap': 0.4, 'transformations_per_example': 1}
  split_names: {'enabled': True, 'join_parts': True}
  label_exclusion: {'enabled': True, 'excluded_labels': ['Mythology', 'Historical']}
  internal_swap: {'enabled': True, 'swap_probability': 0.6}
labels: ['HarryPotter', 'StarWars', 'Tolkien', 'Warcraft', 'DragonBall', 'Naruto', 'ForgottenRealms', 'FinalFantasy', 'GameofThrones', 'TheWitcher', 'DoctorWho', 'Discworld', 'Mythology', 'Offensive', 'Historical']

Datasets:
  - Wikidata:
      name: Wikidata
      path: wikidata
      input_folder: raw_data
      output_folder: processed_data
      labels_folder: labels
      labels_file: labels.txt
      historical_file: extremely_relevant_figures2.txt
      dataset_file: wikidata-universes.csv
      output_file: wikidata_dataset_FastText.txt
  - Mythdata:
      name: Mythdata
      path: mythology
      input_folder: raw_data
      output_folder: processed_data
      dataset_file: myth_dataset.csv
      output_file: myth_dataset.txt
  - NERdata:
      name: NERdata
      path: ner
      input_folder: raw_data
      output_folder: processed_data
      output_file: ner_dataset.txt
  - Slurs:
      name: Slurs
      path: slurs
      input_folder: raw_data
      output_folder: processed_data
      dataset_file: profanity.csv
      output_file: slurs.txt

